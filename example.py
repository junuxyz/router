"""
RTX 3050 Tiìš© ìµœì†Œ ìˆ˜í•™ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (ë””ë²„ê¹… ë²„ì „)
ëª¨ë¸: Qwen2-1.5B-Instruct (ì•½ 3GB VRAM)
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import re

print("="*80)
print("ğŸš€ í‰ê°€ ì‹œì‘")
print("="*80)

# ============================================================================
# 1. ëª¨ë¸ ë¡œë“œ
# ============================================================================
model_name = "Qwen/Qwen2-1.5B-Instruct"
# Ohter options: 
# - "microsoft/phi-2" (2.7B)
# - "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
# - "google/gemma-2b-it"

print(f"\nğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
print(f"   - CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   - GPU: {torch.cuda.get_device_name(0)}")
    print(f"   - VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    dtype=torch.float16, # for memory efficiency
    device_map="auto",
    low_cpu_mem_usage=True
)

print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters() / 1e9:.2f}B")
print(f"   - ë””ë°”ì´ìŠ¤: {model.device}")

# ============================================================================
# 2. ë°ì´í„° ë¡œë“œ
# ============================================================================
print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
dataset = load_dataset("openai/gsm8k", "main", split="test[:5]")  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ ë¬¸ì œ")

# ============================================================================
# 3. ë‹µë³€ ì¶”ì¶œ í•¨ìˆ˜
# ============================================================================
def extract_answer(text):
    """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
    print(f"      ğŸ” ë‹µë³€ ì¶”ì¶œ ì¤‘...")
    
    # íŒ¨í„´ 1: "answer is X"
    match = re.search(r'answer is[:\s]+(-?\d+\.?\d*)', text.lower())
    if match:
        answer = match.group(1)
        print(f"         âœ“ 'answer is' íŒ¨í„´ì—ì„œ ë°œê²¬: {answer}")
        return answer
    
    # íŒ¨í„´ 2: ë§ˆì§€ë§‰ ìˆ«ì
    numbers = re.findall(r'-?\d+\.?\d*', text)
    if numbers:
        answer = numbers[-1]
        print(f"         âœ“ ë§ˆì§€ë§‰ ìˆ«ì ì‚¬ìš©: {answer}")
        return answer
    
    print(f"         âœ— ìˆ«ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    return None

def get_gold_answer(answer_text):
    """GSM8Kì˜ ì •ë‹µ ì¶”ì¶œ (#### ë’¤ì˜ ìˆ«ì)"""
    match = re.search(r'#### (.+)', answer_text)
    if match:
        return match.group(1).strip().replace(',', '')
    return None

# ============================================================================
# 4. í‰ê°€ ë£¨í”„
# ============================================================================
print(f"\n{'='*80}")
print("ğŸ”„ í‰ê°€ ì‹œì‘")
print(f"{'='*80}\n")

correct = 0
results = []

for idx, example in enumerate(dataset):
    print(f"[{idx+1}/{len(dataset)}] " + "="*70)
    
    # Step 1: ë¬¸ì œ í™•ì¸
    question = example['question']
    print(f"â“ ë¬¸ì œ: {question[:100]}...")
    
    # Step 2: í† í°í™”
    print(f"\n   âš™ï¸  í† í°í™” ì¤‘...")
    inputs = tokenizer(question, return_tensors="pt").to(model.device)
    input_length = inputs['input_ids'].shape[1]
    print(f"      - ì…ë ¥ í† í° ìˆ˜: {input_length}")
    
    # Step 3: ìƒì„±
    print(f"   ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    output_length = outputs.shape[1]
    generated_length = output_length - input_length
    print(f"      - ìƒì„± í† í° ìˆ˜: {generated_length}")
    
    # Step 4: ë””ì½”ë”©
    prediction = tokenizer.decode(
        outputs[0][input_length:], 
        skip_special_tokens=True
    )
    print(f"\n   ğŸ’¬ ìƒì„±ëœ ë‹µë³€:")
    print(f"      {prediction[:200]}...")
    
    # Step 5: ë‹µë³€ ì¶”ì¶œ
    pred_answer = extract_answer(prediction)
    gold_answer = get_gold_answer(example['answer'])
    print(f"\n   ğŸ“ ì •ë‹µ ì¶”ì¶œ:")
    print(f"      - ì •ë‹µ (Gold): {gold_answer}")
    print(f"      - ì˜ˆì¸¡ (Pred): {pred_answer}")
    
    # Step 6: í‰ê°€
    is_correct = 1 if pred_answer == gold_answer else 0
    correct += is_correct
    
    emoji = "âœ…" if is_correct else "âŒ"
    print(f"\n   {emoji} ê²°ê³¼: {'ì •ë‹µ' if is_correct else 'ì˜¤ë‹µ'}")
    print(f"   ğŸ“Š í˜„ì¬ ì •í™•ë„: {correct}/{idx+1} = {correct/(idx+1)*100:.1f}%")
    
    results.append({
        "idx": idx,
        "question": question,
        "prediction": prediction,
        "pred_answer": pred_answer,
        "gold_answer": gold_answer,
        "label": is_correct
    })
    
    print()

# ============================================================================
# 5. ìµœì¢… ê²°ê³¼
# ============================================================================
print(f"{'='*80}")
print("ğŸ“ˆ ìµœì¢… ê²°ê³¼")
print(f"{'='*80}")

accuracy = correct / len(dataset)
print(f"\nâœ¨ ì •í™•ë„: {accuracy:.2%} ({correct}/{len(dataset)})")

print(f"\nğŸ“‹ ìƒì„¸ ê²°ê³¼:")
for r in results:
    emoji = "âœ…" if r['label'] else "âŒ"
    print(f"  {emoji} ë¬¸ì œ {r['idx']+1}: Pred={r['pred_answer']}, Gold={r['gold_answer']}")

# JSON ì €ì¥
import json
with open('debug_results.json', 'w', encoding='utf-8') as f:
    json.dump({
        "model": model_name,
        "accuracy": accuracy,
        "correct": correct,
        "total": len(dataset),
        "predictions": results
    }, f, indent=2, ensure_ascii=False)

print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: debug_results.json")
print(f"\n{'='*80}")
print("âœ… í‰ê°€ ì™„ë£Œ!")
print(f"{'='*80}")
